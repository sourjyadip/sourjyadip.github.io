---
permalink: /
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I’m a MS (by Research) student at IIT Kharagpur’s [Complex Networks Research Group (CNeRG) Lab](https://cnerg-iitkgp.github.io/), advised by [Dr. Pawan Goyal](https://cse.iitkgp.ac.in/~pawang/), where I explore multimodal question answering, answer evaluation, and model interpretability. My current projects span two high-impact domains: (i) Visual QA for healthcare images, benchmarking large vision-language models inside hospitals, and (ii) reasoning-driven QA on lecture videos, helping students obtain richer, long-form explanations from MOOCs. I have been working on fusing language, vision, and reasoning signals from synthetic data to build systems that are both accurate and human-centric.

I’ve released several datasets and papers along this theme—most recently [ERVQA (EMNLP 2024 Main)](https://arxiv.org/abs/2410.06420), a first-of-its-kind hospital VQA benchmark, and EduVidQA, an under-review corpus for generating and assessing detailed answers to student questions on lecture videos. When I’m not debugging models, you’ll likely find me making music, watching films, or watching football/cricket.

# Research Interests
I am interested in working more on visual reasoning at test-time, RL finetuning using synthetic data and interpretability of vision language models. These interests have stemmed from the work I have been a part of, and I aim to explore them further in the near future. If these topics excite you too, I would be keen to collaborate with you. Please reach out to me on my email!
