---
permalink: /
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi! I’m a MS (by Research) student at IIT Kharagpur’s [Complex Networks Research Group (CNeRG) Lab](https://cnerg-iitkgp.github.io/), advised by [Dr. Pawan Goyal](https://cse.iitkgp.ac.in/~pawang/), where I explore multimodal question answering, answer evaluation, and model interpretability. My current projects span two high-impact domains: (i) Visual QA for healthcare images, benchmarking large vision-language models inside hospitals, and (ii) reasoning-driven QA on lecture videos, helping students obtain richer, long-form explanations from MOOCs. I have been working on fusing language, vision, and reasoning signals from synthetic data to build systems that are both accurate and human-centric.

I’ve released several datasets and papers along this theme—most recently [ERVQA (EMNLP 2024 Main)](https://arxiv.org/abs/2410.06420), a first-of-its-kind hospital VQA benchmark, and [EduVidQA (EMNLP 2025 Main)](https://aclanthology.org/2025.emnlp-main.1760.pdf), a benchmark for generating and assessing detailed answers to student questions on lecture videos. When I’m not debugging models, you’ll likely find me making music, watching films, or watching football/cricket.

# Research Interests
More recently, I’m excited about agentic simulation / world models: developing interpretable and causal mechanisms for understanding agent behavior, and designing stakeholder-aligned evaluation that supports process-level diagnosis (not just end-task scores). If these topics excite you too, I would be keen to collaborate with you. Please reach out to me on my email!

# Research Updates
- **MS (by Research) expected thesis submission:** January 2026
- **EMNLP 2025 (Main) paper accepted:** *EduVidQA: Generating and Evaluating Long-form Answers to Student Questions based on Lecture Videos*   
- **Teaching Assistant:** Social Computing (CS60017), Autumn 2025
- **Teaching Assistant:** Deep Learning (CS60010), Spring 2025



